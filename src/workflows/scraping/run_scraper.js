/* eslint-disable no-await-in-loop */
// run-scraper.js
import { execa } from 'execa';
import { promises as fs } from 'fs';
import path from 'path';

const SCRAPER_SCRIPT_PATH = path.resolve('src', 'workflows', 'scraping', 'index.js');
const LINKS_FILE = 'links.json';

/**
 * Reads and parses the links.json file.
 * @returns {Promise<Array<object>>}
 */
async function loadLinks() {
  try {
    const data = await fs.readFile(LINKS_FILE, 'utf-8');
    return JSON.parse(data);
  } catch (error) {
    console.error('[-] Error reading or parsing links.json:', error.message);
    process.exit(1);
  }
}

/**
 * Writes the updated links array back to links.json
 * @param {Array<object>} links
 */
async function saveLinks(links) {
  try {
    await fs.writeFile(LINKS_FILE, JSON.stringify(links, null, 2), 'utf-8');
  } catch (error) {
    console.error('[-] Error writing links.json:', error.message);
  }
}

/**
 * Extracts the JSON filename generated by the scraper from the console output.
 * Assumes the scraper logs "[+] Saved scraped data to <filename>".
 * @param {string} output
 */
function extractJsonFilename(output) {
  const match = output.match(/\[\+\] Saved scraped data to (.+\.json)/);
  return match ? path.basename(match[1]) : null;
}

/**
 * The main orchestrator function.
 */
async function main() {
  console.log('[i] Starting the scraping orchestrator...');
  const links = await loadLinks();

  const jobsToProcess = links.filter(job => job.Status !== 'COMPLETED');
  const totalLinks = links.length;
  const linksLeftToProcess = jobsToProcess.length;

  console.log(`[i] Found ${totalLinks} links in total.`);
  console.log(`[i] ${linksLeftToProcess} links left to process.`);
  
  let count = 1;
  for (const job of links) {
    if (job.Status === 'COMPLETED') {
      console.log(`\n[i] Skipping completed job: Tag "${job.SL}"`);
      continue; // skip already completed jobs
    }

    console.log('\n======================================================');
    console.log(`[*] Starting job ${count}/${linksLeftToProcess}: Tag "${job.SL}"`);
    console.log(`[*] Link: ${job.Link}`);
    console.log('======================================================');

    try {
      const args = ['--link', job.Link, '--tag', job.SL];

      // Capture output to extract JSON filename
      const subprocess = execa('node', [SCRAPER_SCRIPT_PATH, ...args]);
      let outputLog = '';
      subprocess.stdout.pipe(process.stdout);
      subprocess.stderr.pipe(process.stderr);
      subprocess.stdout.on('data', chunk => (outputLog += chunk.toString()));
      subprocess.stderr.on('data', chunk => (outputLog += chunk.toString()));

      await subprocess;

      console.log(`[+] Job ${count}/${linksLeftToProcess} for tag "${job.SL}" completed successfully.`);

      // Extract JSON filename
      const jsonFile = extractJsonFilename(outputLog) || '';

      // Update job status and file
      job.Status = 'COMPLETED';
      job.File = jsonFile;

      // Save updated links.json after each job
      await saveLinks(links);
    } catch (error) {
      console.error(`\n[-] Job ${count}/${linksLeftToProcess} for tag "${job.SL}" failed.`);
      console.error('[-] Moving to the next job...');
    }

    count++;
  }

  console.log('\n[+] All scraping jobs have been processed.');
}

main();